{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea45fcc9",
   "metadata": {},
   "source": [
    "# Extracting Sentinel-2 indices time series from Google Earth Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c19a1-0129-49d4-a395-68fc01228a7b",
   "metadata": {},
   "source": [
    "# Step 1. Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691af73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import geemap\n",
    "import ee\n",
    "import numpy\n",
    "import eemont\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import pandas\n",
    "from osgeo import ogr\n",
    "\n",
    "print('\\nGoogle Earth Engine initialize..')\n",
    "ee.Initialize()\n",
    "print('Done.\\n')\n",
    "\n",
    "Map = geemap.Map(center=[41.8,12.5], zoom=2)\n",
    "url = 'http://mt0.google.com/vt/lyrs=y&hl=en&x={x}&y={y}&z={z}'\n",
    "Map.add_tile_layer(url, name='Google Map Hybrid', attribution='Google')\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f9e5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 2. Input 4 mandatory parameters defined by the user: \n",
    "\n",
    "### Path of fields of interest file\n",
    "\n",
    "POLYGON_PATH: [\"DRAW\" | shapefile path | geoson file path]\n",
    "\n",
    "Note: Choose \"DRAW\" to draw the polygon on the MAP ABOVE or write the path of a shapefile (.shp) or a .geojson file. If you chose \"DRAW\", draw the polygon/polygons on the map before moving forward.\n",
    "\n",
    "### Date range (initial and final dates of the analysis)\n",
    "\n",
    "date1: ['YYYY-MM-DD'] & date2: ['YYYY-MM-DD']\n",
    "\n",
    "### Path of output file with Vegetation Indices statistics\n",
    "\n",
    "OUTPUT_CSV: [path of output .csv files]\n",
    "\n",
    "### Anomaly type\n",
    "\n",
    "anomaly: ['true' | 'false' | 'topredict']\n",
    "\n",
    "Note: \n",
    "Choose anomaly 'true' or 'false' to create the datasets required for the model training ['true' for fields related to the presence of an anomaly (i.e. cause by pest) or 'false' in case are not related to any anomaly], \n",
    "Or choose anomaly 'topredict' in case the fields need to be analyzed based on a already trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11166535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS to be defined by the user\n",
    "\n",
    "# Define Fields of Interest File path\n",
    "# *** write \"DRAW\" if you want to draw the polygon on the MAP ABOVE\n",
    "# *** or write the path of a .SHP or .GEOJSON file\n",
    "POLYGON_PATH= r'C:\\Users\\Utente\\Documents\\PSA\\test\\roi\\wheat_tests.shp'\n",
    "\n",
    "# SELECT THE INPUT DATES\n",
    "date1='2022-10-01' # year-month-day FORMAT 'YYYY-MM-DD'\n",
    "date2='2022-12-01' # year-month-day FORMAT 'YYYY-MM-DD'\n",
    "\n",
    "# SELECT ANOMALY PRESENCE\n",
    "anomaly='true' # 'true' or 'false' or 'topredict'\n",
    "\n",
    "# Define CSV output directory (path with '/' at the end)\n",
    "OUTDIR_CSV='C:/Users/Utente/Documents/PSA/test/CSV/'\n",
    "\n",
    "#------------------------------------------------------------------------------------------#\n",
    "print('SELECTED PARAMETERS\\n')\n",
    "print('    INPUT POLYGON_PATH:  {}'.format(POLYGON_PATH) )\n",
    "# if you chose \"DRAW\", draw the polygon/polygons on the previous map before moving forward\n",
    "if POLYGON_PATH==\"DRAW\":\n",
    "    print('If you chose \"DRAW\", draw the polygon/polygons on the map before moving forward')\n",
    "    roiname='roi'\n",
    "else:\n",
    "    roiname=os.path.splitext(os.path.basename(POLYGON_PATH))[0]\n",
    "print('    ROI name : {}\\n'.format(roiname))\n",
    "print('    INPUT DATE RANGE: {} - {}\\n'.format(date1,date2) )\n",
    "print('    ANOMALY: {}\\n'.format(anomaly) )\n",
    "print('Step finished.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0eed43",
   "metadata": {},
   "source": [
    "# Step 3. Region of interest visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1e63a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CASE 1 : DRAW YOUR REGION OF INTEREST (POLYGON) ON THE MAP ABOVE\n",
    "if POLYGON_PATH==\"DRAW\": # Before running this part you have to select and draw at least 1 polygon on the previous Map\n",
    "    Map.draw_features\n",
    "    roi = ee.FeatureCollection(Map.draw_features)\n",
    "    feat_collection=ee.FeatureCollection(roi)\n",
    "    roi_geojson=geemap.ee_to_geojson(roi)\n",
    "    #print(roi_geojson)\n",
    "    geemap.ee_to_geojson(roi, filename='Region_of_interest.geojson')\n",
    "    roi_path='Region_of_interest.geojson'\n",
    "    \n",
    "# CASE 2: Using a SHAPEFILE AS REGION OF INTEREST\n",
    "elif POLYGON_PATH.endswith('.shp'):\n",
    "    path_shp =POLYGON_PATH\n",
    "    roi=geemap.shp_to_ee(path_shp)\n",
    "    feat_collection=ee.FeatureCollection(geemap.shp_to_ee(path_shp))\n",
    "    roi_path=path_shp\n",
    "    \n",
    "# CASE 3: Using a GEOJSON FILE AS REGION OF INTEREST\n",
    "elif POLYGON_PATH.endswith('.geojson'):\n",
    "    geojson_path='Region_of_interest.geojson'\n",
    "    roi=geemap.geojson_to_ee(geojson_path)\n",
    "    feat_collection=ee.FeatureCollection(roi)\n",
    "    roi_path=geojson_path\n",
    "\n",
    "print('Done')\n",
    "    \n",
    "Map.clear()\n",
    "Map.add_tile_layer(url, name='Google Map Hybrid', attribution='Google')\n",
    "Map.addLayer(feat_collection)\n",
    "Map.centerObject(feat_collection, 16)\n",
    "Map    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa662d",
   "metadata": {},
   "source": [
    "# Step 4. Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def addIndices(img):\n",
    "    N =  img.select('B8')\n",
    "    R =  img.select('B4') \n",
    "    B =  img.select('B2') \n",
    "    RE1 =  img.select('B5') \n",
    "    RE2 =  img.select('B6') \n",
    "    G =  img.select('B3') \n",
    "    modNDRE = ee.Image().expression(\n",
    "             \"(N-RE1)/(N+RE1-(2**B))\",\n",
    "             {\"N\": N, \"RE1\": RE1, \"B\": B} ).rename(\"modNDRE\")\n",
    "    PVR = ee.Image().expression(\n",
    "             \"(G/R)\",\n",
    "             {\"G\": G, \"R\": R} ).rename(\"PVR\")\n",
    "    GCI = ee.Image().expression(\n",
    "             \"(N/G) - 1\",\n",
    "             {\"G\": G, \"N\": N} ).rename(\"GCI\")\n",
    "    LCI = ee.Image().expression(\n",
    "             \"(N-RE1) / (N+R)\",\n",
    "             {\"R\": R,\"RE1\": RE1, \"N\": N} ).rename(\"LCI\")\n",
    "    TCVI = ee.Image().expression(\n",
    "             \"-0.283*G - 0.660*R + 0.577*RE1 + 0.388*N\",\n",
    "             {\"R\": R,\"RE1\": RE1, \"N\": N, \"G\": G} ).rename(\"TCVI\")\n",
    "    SIPI1 = ee.Image().expression(\n",
    "             \" (N-B)/(N-R)\",\n",
    "             {\"R\": R,\"B\": B, \"N\": N} ).rename(\"SIPI1\")\n",
    "    SIPI2 = ee.Image().expression(\n",
    "             \" (N-B)/(N-R)\",\n",
    "             {\"R\": R,\"B\": B, \"N\": N} ).rename(\"SIPI2\")   \n",
    "    return img.addBands([modNDRE,PVR,GCI,LCI,TCVI,SIPI1,SIPI2])                    \n",
    "\n",
    "def getlist_unique_fromdataframe(df,field_str):\n",
    "    df_date=df[field_str]\n",
    "    dlist=[]\n",
    "    for i in range(0, len(df_date)):\n",
    "        update=df_date[i]\n",
    "        if update not in dlist:\n",
    "            dlist.append(update)    \n",
    "    return dlist\n",
    "\n",
    "def getindexlist_fromdataframe_eqvalue(df,field_str,value):\n",
    "    df_date=df[field_str]\n",
    "    indexlist=[]\n",
    "    for i in range(0, len(df_date)):\n",
    "        update=df_date[i]\n",
    "        if update == value:\n",
    "            indexlist.append(i)    \n",
    "    return indexlist\n",
    "\n",
    "def getindexlist_fromdataframe_includestring(df,field_str,value):\n",
    "    df_date=df[field_str]\n",
    "    indexlist=[]\n",
    "    for i in range(0, len(df_date)):\n",
    "        update=df_date[i]\n",
    "        if value in update:\n",
    "            indexlist.append(i)    \n",
    "    return indexlist\n",
    "\n",
    "def getindexlist_fromdataframe_filter_date_ID(df,date_,id_):\n",
    "    idxlist=[]\n",
    "    dateindex=getindexlist_fromdataframe_eqvalue(df,'date',date_)\n",
    "    idindex=getindexlist_fromdataframe_eqvalue(df,'id',id_) \n",
    "    idxlist= extract_common_list(dateindex,idindex)\n",
    "    return idxlist\n",
    "\n",
    "def getindexlist_fromdataframe_filter_date_ID_reducer(df,date_,id_, reducer_):\n",
    "    idxlist=[]\n",
    "    dateindex=getindexlist_fromdataframe_eqvalue(df,'date',date_)\n",
    "    idindex=getindexlist_fromdataframe_eqvalue(df,'id',id_) \n",
    "    redindex=getindexlist_fromdataframe_eqvalue(df,'reducer',reducer_) \n",
    "    list_0=extract_common_list(dateindex, idindex)\n",
    "    idxlist= extract_common_list(list_0,redindex)\n",
    "    return idxlist\n",
    "\n",
    "def getindexlist_fromdataframe_filter_dateday_ID_reducer(df,date_,id_,reducer_):\n",
    "    idxlist=[]\n",
    "    dateindex=getindexlist_fromdataframe_includestring(df,'date',date_)\n",
    "    idindex=getindexlist_fromdataframe_eqvalue(df,'id',id_) \n",
    "    redindex=getindexlist_fromdataframe_eqvalue(df,'reducer',reducer_) \n",
    "    list_0=extract_common_list(dateindex, idindex)\n",
    "    idxlist= extract_common_list(list_0,redindex)\n",
    "    return idxlist\n",
    "\n",
    "def extract_common_list(list1, list2):\n",
    "    commonlist=[]\n",
    "    for i in range(0, len(list1)):\n",
    "        update=list1[i]\n",
    "        if update in list2 :\n",
    "            commonlist.append(update)    \n",
    "    return commonlist\n",
    "\n",
    "def correct_nan_values(mean_,min_,max_,std_):\n",
    "    if numpy.isnan(mean_ ) or numpy.isnan(min_ )or numpy.isnan(max_ )or numpy.isnan(std_ ) :\n",
    "        mean_=numpy.nan\n",
    "        min_=numpy.nan\n",
    "        max_=numpy.nan\n",
    "        std_=numpy.nan\n",
    "    return mean_,min_,max_,std_\n",
    "\n",
    "def get_unique_date_by_day_list(datelist_):\n",
    "    filtered_datelist=[]\n",
    "    for i in range(0, len(datelist_)):\n",
    "        update=datelist_[i][0:10]\n",
    "        if update not in filtered_datelist :\n",
    "            filtered_datelist.append(update) \n",
    "    return filtered_datelist\n",
    "\n",
    "def get_unique_date_by_datetime_list(datelist_):\n",
    "    filtered_datelist=[]\n",
    "    for i in range(0, len(datelist_)):\n",
    "        update=datelist_[i]\n",
    "        if update not in filtered_datelist :\n",
    "            filtered_datelist.append(update) \n",
    "    return filtered_datelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751980e",
   "metadata": {},
   "source": [
    "# Step 5. GEE image collection (Sentinel-2) processing\n",
    "### Extraction of Field Statitics (Mean, Minimum, Maximum, Standard Deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEE image collection (Sentinel-2) + Preprocessing + Statitics extraction\n",
    "\n",
    "# ROI \n",
    "feat_collection=roi\n",
    "\n",
    "# INDICES\n",
    "# Supported by spectralIndices library in GEE eemont\n",
    "spectralIndices_list= [ \"NDVI\",\"SR\",\"GNDVI\",\"NDREI\",\"EVI\",\"EVI2\",\"CIG\",\"CIRE\",\"TriVI\",\"MTCI\",\"GARI\",\"SIPI\",\"MCARI\",\"ARVI\", \"OSAVI\"]\n",
    "# Unsupported by spectralIndices library, builded by the function addIndices()\n",
    "addIndices_list = [\"modNDRE\",\"PVR\",\"GCI\",\"LCI\",\"TCVI\",\"SIPI1\",\"SIPI2\"]\n",
    "# FInal order of the CSV visualization (all indexes)\n",
    "all_index_list= [ \"SR\",\"NDVI\",\"GNDVI\",\"NDREI\",\"modNDRE\",\"EVI\",\"EVI2\",\"PVR\",\"GCI\",\"CIRE\",\"TriVI\",\"MTCI\",\"LCI\",\"TCVI\",\"GARI\",\"SIPI1\",\"SIPI2\",\"MCARI\",\"ARVI\", \"OSAVI\"]\n",
    "\n",
    "# Google Earth Engine: get IMAGE COLLECTION (SENTINEL-2) ----------------------\n",
    "# A Sentinel-2 surface reflectance image.\n",
    "print('Collecting Sentinel-2 images from GEE... ')\n",
    "ds = 'COPERNICUS/S2_SR'         \n",
    "S2 = (ee.ImageCollection(ds)\n",
    "     #.closest(date).first().scale().filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', 40)).maskClouds(prob = 75,buffer = 300,cdi = -0.5)\n",
    "    .filterBounds(feat_collection)\n",
    "    .scaleAndOffset()\n",
    "    .filterDate(date1, date2)\n",
    "    .maskClouds()\n",
    "    .spectralIndices(spectralIndices_list)\n",
    "    .map(addIndices) )\n",
    "print('Done.\\n')\n",
    "\n",
    "# Get statistics --------------------------------------------------------------\n",
    "print('Extracting zonal statistics time-series from GEE image-collection ...')\n",
    "ts = S2.getTimeSeriesByRegions(reducer = [ee.Reducer.mean(),ee.Reducer.min(),ee.Reducer.max(),ee.Reducer.stdDev()], \n",
    "                              collection = feat_collection,\n",
    "                              bands = all_index_list,\n",
    "                              naValue = -9999,\n",
    "                              scale = 10)\n",
    "print('Step finished.\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a87d0",
   "metadata": {},
   "source": [
    "# Step 6. Downloading of fields statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2998900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and Reading stats as pandas Dataframe.\n",
    "\n",
    "dirpath=OUTDIR_CSV\n",
    "if not os.path.isdir(dirpath):\n",
    "    os.mkdir(dirpath)   \n",
    "print('Downloading and Reading stats as pandas Dataframe...')\n",
    "# gdf = geemap.ee_to_pandas(ts)\n",
    "url = ts.getDownloadURL(\n",
    "    filetype=\"csv\",\n",
    "    selectors=['id','reducer','date',\"NDVI\",\"SR\",\"GNDVI\",\"NDREI\",\"modNDRE\",\"EVI\",\"EVI2\",\"PVR\",\"GCI\",\"CIRE\",\"TriVI\",\"MTCI\",\"LCI\",\"TCVI\",\"GARI\",\"SIPI1\",\"SIPI2\",\"MCARI\",\"ARVI\",\"OSAVI\"],#+all_index_list\n",
    "    filename=\"ts\")\n",
    "print(url)\n",
    "temp_path=os.path.join(OUTDIR_CSV,'csv_from_url.csv')\n",
    "response = requests.get(url)\n",
    "print('CSV written.\\n')\n",
    "with open(temp_path, 'wb') as fd:\n",
    "    fd.write(response.content)  \n",
    "print('Reference CSV path : {}'.format(temp_path))\n",
    "print('Step finished.\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2a428",
   "metadata": {},
   "source": [
    "# Step 7. Export fields statistics\n",
    "### output: fields_anomaly_[anomaly].csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a37a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing CSV file\n",
    "# Manipulating the information inside the pandas dataframe to obtain the desired CSV visualization and format\n",
    "\n",
    "gdf=pandas.read_csv(temp_path)\n",
    "gdf[gdf == -9999] = numpy.nan\n",
    "datelist=(getlist_unique_fromdataframe(gdf,'date'))\n",
    "#print('Date list : {}'.format(datelist))\n",
    "\n",
    "# Open the shapefile and get the first layer\n",
    "datasource = ogr.Open(roi_path)\n",
    "layer = datasource.GetLayerByIndex(0)\n",
    "featurecount=layer.GetFeatureCount()\n",
    "print(\"Number of features:{}\".format(layer.GetFeatureCount()))\n",
    "Layer=None\n",
    "datasource=None\n",
    "length=len(gdf['date'])/4/featurecount\n",
    "idlist=getlist_unique_fromdataframe(gdf,'id')\n",
    "number_of_rows_per_stats=gdf['reducer'].loc[lambda x: x=='min'].index[0]\n",
    "img_number=int(len(getlist_unique_fromdataframe(gdf,'date')))\n",
    "multiple_number_stats=int((len(gdf['date'])/4))\n",
    "num_of_polygons=int(featurecount)\n",
    "uniquedaylist=get_unique_date_by_day_list(getlist_unique_fromdataframe(gdf,'date'))\n",
    "print('Date list : {}'.format(uniquedaylist))\n",
    "\n",
    "#check ID values\n",
    "if numpy.nan in gdf['id']:\n",
    "    id_column=numpy.arange(0,int((num_of_polygons))).tolist()*(len(idlist))\n",
    "    gdf['id']=id_column\n",
    "    idlist=getlist_unique_fromdataframe(gdf,'id')\n",
    "\n",
    "#check NaN values    \n",
    "for z in gdf['id']:\n",
    "    if numpy.isnan(z) :\n",
    "        id_column=numpy.arange(0,int((num_of_polygons))).tolist()*(len(idlist))\n",
    "        gdf['id']=id_column\n",
    "        idlist=getlist_unique_fromdataframe(gdf,'id')\n",
    "        break\n",
    "        \n",
    "print('Writing CSV file...')\n",
    "all_index_list2 = [word.replace('NDREI','NDRE') for word in all_index_list]\n",
    "all_index_list2 = [word.replace('CIRE','RECI') for word in all_index_list2]\n",
    "all_index_list2 = [word.replace('TriVI','TVI') for word in all_index_list2]\n",
    "all_index_list2 = [word.replace('GARI', 'GARVI') for word in all_index_list2]         \n",
    "basenamecsv='fields_anomaly_' + anomaly\n",
    "filepathcsv=os.path.join(OUTDIR_CSV,basenamecsv+'.csv')\n",
    "if os.path.isfile(filepathcsv):\n",
    "    os.remove(filepathcsv)   \n",
    "# creation of Header\n",
    "list_fields=['date']\n",
    "for key in all_index_list2:\n",
    "    list_fields=list_fields+list([key+'_MIN'])+list([key+'_MAX'])+list([key+'_MEAN'])+list([key+'_STD'])            \n",
    "list_fields.append('anomaly')\n",
    "with open(filepathcsv, 'a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(list_fields)\n",
    "\n",
    "for k in idlist:\n",
    "    #filepathcsv=os.path.join(OUTDIR_CSV,basenamecsv+str(k)+'.csv')        \n",
    "    daylist=[]\n",
    "    for date in datelist:\n",
    "        day=date[0:10]\n",
    "        if not day in daylist:\n",
    "            daylist.append(day)\n",
    "            list_csv=[]\n",
    "            list_csv.append(date[0:10])#\n",
    "            for key in all_index_list:\n",
    "                    idx0=getindexlist_fromdataframe_filter_dateday_ID_reducer(gdf,date,k,'mean')[0]\n",
    "                    meanv= gdf[key][idx0]\n",
    "                    minv= gdf[key][idx0+multiple_number_stats]\n",
    "                    maxv= gdf[key][idx0+multiple_number_stats*2]\n",
    "                    stdv= gdf[key][idx0+multiple_number_stats*3]\n",
    "                    meanv, minv, maxv, stdv=correct_nan_values(meanv,minv,maxv,stdv)\n",
    "                    list_csv.append(minv)\n",
    "                    list_csv.append(maxv)\n",
    "                    list_csv.append(meanv)\n",
    "                    list_csv.append(stdv)\n",
    "            list_csv.append(anomaly)\n",
    "            with open(filepathcsv, 'a', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(list_csv)\n",
    "print('File written:  {}\\n'.format(filepathcsv))\n",
    "print('Step finished. Output .csv file ready. \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psaenv",
   "language": "python",
   "name": "psaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
